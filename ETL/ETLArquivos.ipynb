{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_db(user, password):\n",
    "    dbServerName         = 'minhabase.database.windows.net'\n",
    "    dbName               = 'nomebanco'\n",
    "    dbUserName           = \"username\"\n",
    "    dbPassword           = dbutils.secrets.get (scope= 'scope', key = 'svr-secret')\n",
    "    driver               = 'com.microsoft.sqlserver.jdbc.SQLServerDriver'\n",
    "    server_name = f\"jdbc:sqlserver://{dbServerName}:1433;database={dbName}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data_to_sql(dataframe, table_name, connection):\n",
    "\n",
    "    dataframe.to_sql(table_name, connection, if_exists='append', index=False)\n",
    "\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime(connection, table_name, column_name):\n",
    "    query = f\"SELECT [{column_name}] FROM {table_name}\"\n",
    "    dataframe = pd.read_sql(query, connection)\n",
    "\n",
    "    dataframe[column_name] = pd.to_datetime(dataframe[column_name], errors='coerce')\n",
    "\n",
    "    dataframe.to_sql(table_name, connection, if_exists='replace', index=False)\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numeric(connection, dataframe, col_name):\n",
    "    query = f\"SELECT [{column_name}] FROM {table_name}\"\n",
    "    dataframe = pd.read_sql(query, connection)\n",
    "\n",
    "    dataframe[column_name] = pd.to_int(dataframe[column_name], errors='coerce')\n",
    "\n",
    "    dataframe.to_sql(table_name, connection, if_exists='replace', index=False)\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(dataframe, col_name):\n",
    "    if col_name not in dataframe.columns:\n",
    "        raise ValueError(f\"A coluna '{col_name}' n√£o existe no DataFrame.\")\n",
    "\n",
    "    replacements = {\"1\": None, \"Mr.\": \"Sr.\", \"Ms.\": \"Sra.\", \"Ms\": \"Sra.\", \"Mrs.\": \"Sra.\"}\n",
    "\n",
    "    dataframe[col_name] = dataframe[col_name].replace(replacements)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mount(\\\n",
    "    source = \"wasbs://camilayzidoro@blobname.blob.core.windows.net\",\\\n",
    "    mount_point = \"/mnt/camilayzidoro\",\\\n",
    "    extra_configs = {\"fs.azure.account.key.blobname.blob.core.windows.net\":dbutils.secrets.get(scope = 'scope', key = \"stg-secret\")})\n",
    "\n",
    "\n",
    "mount_point = \"/mnt/camilayzidoro\"\n",
    "\n",
    "df_person = spark.read.format(\"csv\").options(header='true', delimiter = ';').load(f\"{mount_point}/Person.Person.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace(df_person, 'Title')\n",
    "\n",
    "print(df_person) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data_to_sql(df_person, '[Person].Person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = \"/mnt/camilayzidoro\"\n",
    "\n",
    "df_product = spark.read.format(\"csv\").options(header='true', delimiter = ';').load(f\"{mount_point}/Production.Product.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_datetime(connection_db, df_product, 'SellStartDate')\n",
    "convert_numeric(connection_db, df_product, 'ProductSubcategoryID')\n",
    "convert_numeric(connection_db, df_product, 'ProductLine')\n",
    "convert_numeric(connection_db, df_product,'DaysToManufacture')\n",
    "convert_numeric(connection_db, df_product, 'Weight')\n",
    "convert_numeric(connection_db, df_product,'SafetyStockLevel')\n",
    "replace(df_product,'Color')\n",
    "\n",
    "print(df_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data_to_sql(df_product, '[Production].product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mount_point = \"/mnt/camilayzidoro\"\n",
    "\n",
    "df_salesCustomer = spark.read.format(\"csv\").options(header='true', delimiter = ';').load(f\"{mount_point}/Sales.Customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data_to_sql(df_salesCustomer, '[Sales].Customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = \"/mnt/camilayzidoro\"\n",
    "\n",
    "df_specialOfferProduct = spark.read.format(\"csv\").options(header='true', delimiter = ';').load(f\"{mount_point}/Sales.SpecialOfferProduct.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data_to_sql(df_specialOfferProduct, '[Sales].SpecialOfferProduct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = \"/mnt/camilayzidoro\"\n",
    "\n",
    "df_salesOrderHeader = spark.read.format(\"csv\").options(header='true', delimiter = ';').load(f\"{mount_point}/Sales.SalesOrderHeader.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data_to_sql(df_salesOrderHeader, '[Sales].SalesOrderHeader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = \"/mnt/camilayzidoro\"\n",
    "\n",
    "df_salesOrderDetail = spark.read.format(\"csv\").options(header='true', delimiter = ';').load(f\"{mount_point}/Sales.SalesOrderDetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data_to_sql(df_salesOrderDetail, '[Sales].SalesOrderDetail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(\"/mnt/camilayzidoro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
